\chapter{\docname}
\label{\docname}

\section{Allgemeines}


Diese Diplomarbeit besteht aus vielen unterschiedlichen Modulen, die um bestimmte Ziele bzw. Aufgaben zu lösen gut aufgeteilt sind.\\
Sehr wichtiger Teil dieses Projektes ist der Bildverarbeitungsteil.\\
In dem folgenden Abschnitt der Ausarbeitung wird es genau erklärt und beschrieben wie diese Aufgabe gelöst wurde und was dafür verwendet wurde.\\ 
Für die Umsetzung wurden folgende Technologien gebraucht. \\
\\
\subsection{Entwicklungsumgebung und Technologien}

Die gewählte Entwicklungsumgebung war grundsätzlich eine virtuelle Maschine die Linux auf einem Windows System geboten hat. \\
Linux im Gegensatz von Windows ermöglicht volle Kontrolle über Updates und Upgrades und dadurch könnten komplexe Aufgaben einfacher erlediget werden.\\ Alles wird leicht und bequem durch Konsole eingegeben. \\
So wurden die Entwicklung, das Testen von den genutzten Algorithmen und anderen Technologien vielmals erleichtert. \cite{linx}\\

Zusätzlich ist Raspberry Pi als Backup System verwendet worden das auch mit einem lauffähigen Linux Betriebssystem (Debian) funktioniert. \\
Raspberry Pi ist klein, funktioniert aber wie ein normaler Rechner und ist  kostengünstig. Für technische/elektronische Projekte wie das Betreffende, ist es perfekt geeignet. \\ 
Für die Implementierung des Codes wurde hauptsächlich die Programmiersprache Python verwendet. \\
Python ist eine allgemeine Programmiersprache auf hohem Niveau. Dies bedeutet dass es näher an menschlichen Sprachen ist. \\
Also ist ein in Python geschriebener Code sehr leicht von einem Mensch zu lesen, zu verwalten und zu warten.  \\ 
\\
Es bietet zahlreiche Modulen durch seine große und robuste Standardbibliothek an, von denen man ruhig, abhängig vom Bedarf, auswählen kann.  
Sehr leicht kann es in der Dokumentation der Python-Standardbibliothek nachgesehen werden um sich besser mit den gezielten Funktionalitäten auszukennen. \\ \cite{why_python}

"Git ist ein freies und Open Source verteiltes Versionskontrollsystem, das entwickelt wurde, um alles von kleinen bis zu sehr großen Projekten mit Geschwindigkeit und Effizienz abzuwickeln." \cite{Git1} \\
Die Versionierung der ganzen Software Änderungen erfolgte durch Git.\\
Es hat sich nützlich erweist indem die Arbeit dadurch zwischen die Projektmitgliedern sehr gut koordiniert und verwaltet wurde. \\ 


\subsection{Frameworks und Bibliotheken}

Schlüsselwort von unserem Projekt war das Framework bzw. die Bibliothek „OpenCV“.\\
OpenCV ist eine open-source Bibliothek für Computer Vision. Also, ganz allgemein kann sie als eine Bibliothek für Bildverarbeitung betrachtet werden.\\
Sie kümmert sich unter anderem um die Manipulation von Bildern, die Analyse von denen und um die daraus bestimmte Muster bzw. Objekte, für verschiedenen Zwecken einzusetzen. Ganz berühmte Anwendungsgebieten sind Gesichtserkennung und Stereo Vision. \\
Stereo Vision bedeutet die Extrahierung von Informationen aus einem Bild in eine 3-dimensionalen Ebene. \\
OpenCV wurde unter anderen Bibliotheken aufgrund seiner vielen guten Eigenschaften und seiner Flexibilität ausgewählt. Es umfasst hunderte von Computer-Vision-Algorithmen und besteht aus strukturierten Einheiten bzw. Module die als feststehende Bibliotheken implementiert sind. \\
Es ist Cross-Platform, in C/C++ geschrieben und unterstützt auch Python. \\
\cite{opencv_library}
\\
\begin{figure}[H]
	\includegraphics[width=50mm,scale=0.5]{\ordnerfigures opencv.png}
	\caption{OpenCV Logo}
	\label{fig:OpenCV logo} 
	\cite{OpencvLogo}
\end{figure}


SciPy hat sich nützlich, beim Lösen von gewisse mathematische Angabenund bei zusätzliche Installationen von Bibliotheken zu helfen.\\ 
"SciPy ist eine kostenlose und Open-Source-Python-Bibliothek für wissenschaftliches und technisches Rechnen."\cite{2019arXiv190710121V-scipy}
 \\ 
Dies sind einige der Kernpakete von SciPy die nutzbar für das Projekt waren:\\ 
 \\ 
NumPy legt die Basis für das wissenschaftliche Rechnen mit Python.\\ Es unterstützt große mehrdimensionale Arrays und Matrizen. Es enthält viele Mathematische Funktionen auf hoher Ebene um die Arrays zu bearbeiten.\\
NumPy kann also als leistungsfähiger mehrdimensionaler Behälter für generische Daten verwendet werden.\\ Es können beliebige Datentypen definiert werden.
\\
\begin{figure}[H]
	\includegraphics[width=50mm,scale=0.5]{\ordnerfigures numpy.png}
	\caption{Numpy logo}
	\label{fig:Numpy logo}
	\cite{Numpy}
\end{figure}

Dlib ist ein so genanntes Toolkit, das Algorithmen für Machine Learning und Tools zum Erstellen komplexer Software zur Lösung von der realen Welt getauchte Probleme, beinhaltet.\\
Es wird vielseitig eingesetzt, in Robotik, Mobilgeräte und unter anderem in Computer Vision.\cite{dlib} \\
Es wurde prinzipiell bei der Extrahierung der so genannten „Facial Landmarks“ gebraucht. 

\begin{figure}[H]
	\includegraphics[width=50mm,scale=0.5]{\ordnerfigures dlib.png}
	\caption{dlib logo}
	\cite{dlib}
	\label{fig:dlib logo}
\end{figure}


\section{Technische Lösungen}

Bei der Gesichtsregistrierung und Gesichtserkennung Diplomarbeit war es die schwierigste und herausforderndste Aufgabe, sie sorgfältig zu planen, um die Effektivität bei der Arbeit zu steigen und das Endprodukt zufriedenstellend zu machen. \\
 \\
Wichtig war es die Ziele richtig zu setzen und sie gut abzugrenzen damit es zu keine Lücken bei der Implementierung kommt. \\\\
Aus diesem Grund musste die Arbeitsteilung gut geregelt werden, damit jedes Teammitglied sich auf einen bestimmten Modul der Arbeit konzentrieren konnte.\\
Es wurde auch das berücksichtigt, dass jedes Teammitglied das machte was ihm/ihr am besten gefällt und was ihm/ihr am leichtesten fällt. \\ \\
Die Planung der betreffenden Bereiche der Projektarbeit hat durch die Methode „Structed Design“ erfolgt. \\
Structed Design ist eine Erweiterung von der „Big Picture“ Methode, die dazu dient, ein technisches System mit ihren Schnittstellen mit außen grob zu beschreiben. \\
Es ist in verschieden Ebenen unterteilt, von außen beginnend. \\ \\
Unten wird die erste Ebene der Structed Design von der Bildverarbeitungsteil dargestellt. \\

\subsection{Lösungsweg - Structed Software design} 
  

\begin{figure}[H]
	\includegraphics[width=\textwidth]{\ordnerfigures bildverarbeitung.png}
	\caption{Bildverarbeitung Structed Design}
	\label{fig:bildverarbeitung}

\end{figure}

Wie es in der Abb.\ref{fig:bildverarbeitung} sichtbar ist, ist die Bildverarbeitungsteil in diese Einheiten unterteilt:
\begin{enumerate}
	\item Image Schaffung
	\item Image Normalisierung
	\item Gesichtschlüsselpunkten Extrahierung
	\item Log
	\item Output 
	\item Trigger \\ \\
	
\end{enumerate}
Die Schnittstellen von außen sind das Gesichtserkennungsmodul und die Gesichtsregistrierungsmodul. \\ 
Diese schicken eine Anforderung an den Trigger der dann das Image Schaffung Modul initialisiert. Unabhängig von welchen von diesen beiden Schnittstellen die Anfrage kommt, macht die Image Schaffung aus der zwei Kameras ein Foto. 
\\
Implementierungsnah wurde bis jetzt nur eine Kamera verwendet, da die Stereo Vision noch nicht funktional ist. \\
Nachdem das Foto gemacht wird folgt die Image Normalisierung bzw. das „Image Prozessing“. \\
Wenn ein Bild zu klein ist wird die Größe angepasst, wenn ein Gesicht verdreht ist wird es gerade rotiert. Affines und perspektivisches Warping wird falls notwendig geführt. Farben werden nach Bedarf auch angepasst usw. \\
Danach erfolgt die Erkennung von Gesichtern, das Ausschneiden von ihnen und die Extrahierung der Gesischtsschlüsselpunkte.\\ Sie werden dann in Vektoren umgewandelt und zur Abgleich oder Registrierung je nach Bedarf, geschickt. \\
\\
\\
Hinweis: Bei der Umsetzung wurden bestimmte Bereiche mit einander verknüpft, was zu Folgendes führte: was bei der Planung steht, stimmt nicht vollig mit der Methoden zur Umsetzung überein. Weitere Unterschiede werden im Punkt 13b. genauer beschrieben.  \\

\subsection{Gesichtsdetektion}

Das erste Ereignis, dass erfüllt werden muss bei der Gesichtserkennung ist das Finden von Gesichtern, also die Feststellung, wo sich die Gesichter im Bild befinden. \\ 
Dafür ist die „Haar Cascade Classifiers“  pre-trained Classifier gebraucht worden. \\ \\
Gesichtsdetektion durch „Haar“ Merkmale ist eine sehr effektive Methode die von Paul Viola und Michael Jones entwickelt wurde, indem sie Merkmale gruppiert haben und die Erkennung von diesen dadurch schneller gemacht haben. \\ \\
Die Arbeit heißt „Rapid Object Detection using a Boosted Cascade of Simple Features". \cite{Viola01robustreal-time}\\

Es geht hier um Maschinelles Lernen, wie diese Kaskadenfunktionen durch viele negative(Bilder ohne Gesichter) und positive(Bilder mit Gesichter) Bilder trainiert wurden um Objekte zu erkennen.\\ \\
In diesem Fall  arbeiten wir selbstverständlich mit Gesicht Objekten. 
Dafür wurden die Haar Merkmale benutzt. Auf Abbildung \ref{fig:haar features} sind sie dargestellt. Jedes Haar Merkmal ist nichts anders als ein Wert, durch das Subtrahieren der Summe der Pixel unter dem weißen Rechteck von der Summe der Pixel unter dem schwarzen Rechteck, erhält. \\
Diese Summen berechnet man durch integrale Bilder, die zur Vereinfachung zu Summenberechnungen dient.
\begin{figure}[H]
	\includegraphics[scale=0.8]{\ordnerfigures haar_features.jpg}
	\caption{Haar Features}
	\label{fig:haar features}
	\cite{Viola01robustreal-time}
\end{figure}

Man muss aufpassen, da nicht alle Merkmale von Nutzen sein könnten. Man kann es zum Beispiel deutlich auf Abbildung \ref*{fig:haar} sehen, wie die Nase viel heller als die Region bei den Augen ist. Die Selektion von den besten Merkmalen wird durch das Adaboost Algorithmus berechnet. 
\begin{figure}[H]
	\includegraphics[scale=0.8]{\ordnerfigures haar.png}
	\caption{Haar Einsetzung}
	\label{fig:haar}
\end{figure}\cite{Viola01robustreal-time}

Mit dieser Absicht setzen wir alle Funktionen auf alle Trainingsbilder ein. Für jedes Merkmal wird der beste Schwellenwert ermittelt, der die Gesichter in positive und negative klassifiziert.\\\\

\subsubsection{Umsetzung in Code}

Durch die Verwendung von classifiers die OpenCV bietet, setzen wir das um. \\\\
\begin{lstlisting}

//haar cascade classifier laden 
1	face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

2	gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)

3	faces = face_cascade.detectMultiScale(gray,1.1,4)

4	for(x, y, w, h) in faces:
//parameter: image, startpunkt,  endpunkt(w..width, h..height), farbe, Dicke der Rahmen
5	cv2.rectangle(image, (x, y), (x+w, y+h), (255,0,0), 2) 


\end{lstlisting}
 //opencv und numpy importieren \\
//Bild als input einfügen\\
//Bild einlesen\\
//Bild in „gray scale“ umwandeln wodurch die Rechenleistung reduziert wird.
//(Da es keine Farben zuhanden sind).\\
//Parametern: src(source image), dst(destination image), code(conversion code)\\
//Hier benutzt man die Cascade Classifier um Gesichter zu finden.\\
//Parameter: \\
//image, objects, scaleFactor = 1.1, minNeighbors = 3, flags = 0, minSize = new cv.Size(0, 0), maxSize = new cv.Size(0, 0)) \\ 
//Zeichenrahmen für das Gesicht zeichnen.\\
//Bild zeigen durch ‘image’ Fenster \\
//Press any key zum schließen vom Fenster\\
\begin{figure}[H]
	\includegraphics[scale=0.6]{\ordnerfigures aronfd.png}
	\caption{Box auf Gesicht}
	\label{fig:aron}
\end{figure}

Die Funktionalität des Code ist genau auf Abb.\ref{fig:aron} ersichtlich. \\

\subsection{Gesichtsdetektion und Zuschneiden}
Nachdem Gesichter gefunden werden, muss man daraus eigene Bilder machen, die zur Erleichterung von der Extrahierung der Schlüsselpunkte dienen.\\\\  
nrFace bestimmt die Anzahl von Gesichter, die von den cascade classifier gefunden wurden. 
Nur wenn dieser Wert größer als Null ist soll die Logik von der Crop Funktion weiterlaufen. \\

\begin{lstlisting}
if nrFace > 0:
for face in faces:
for(x, y, w, h) in faces:
1	r = max(w, h) /2 
2	centerx = x + w /2 
3	centery = y + h /2
4	nx = int(centerx - r) 
5	ny = int(centery - r) 
6	nr = int(r * 2) 
7	faceimg = image[ny:ny+nr, nx:nx+nr] 
filenam = input("Give new filename for cropped photo: \n")
image2 = cv2.imwrite(filenam,faceimg)
elif nrFace <= 0:
print("no faces found")

\end{lstlisting}

Zeilen 1-6 berechnen das Zentrum von Bild neu und in Zeile 7 wird ein neues Image mit den berechneten Parametern angelegt.\\
Mit imwrite wird das Image gespeichert. Auf Abb.\ref{fig:aroncropped} sieht man deutlich nur das Gesicht. \\

\begin{figure}[H]
\includegraphics[scale=0.6]{\ordnerfigures aroncropped.jpg}
\centering
\caption{abgeschnittenes Gesicht}
\label{fig:aroncropped}
\end{figure}

\subsection{Gesichtsschlüsselpunkte Extraktion}


Nun kommt ist es zu dem wichtigsten Punkt meines Teils der Diplomarbeit, die Gesichtsschlüsselpunkte Extraktion. \\\\
Dazu wurden die „Facial Landmarks“ von dlib verwendet. \\
Die Gesichtsmarkierungen werden verwendet, um Bereiche des Gesichts zu finden und darzustellen, wie zum Beispiel: die Augen, die Augenbrauen, die Nase, den Mund und den Kiefer. \\\\
Wie macht man das? Wie erkennt man Landmarken? \\Das ist eine Teilmenge des Problems der Formvorhersage.\\ Bei einem vorgegebenen Eingabebild (und normalerweise einer ROI\footnote{Region of interest}, die das interessierende Objekt angibt) versucht ein Formvorhersager, wichtige Punkte entlang der Form zu lokalisieren. \\
Dieser Formvorhersager die im dlib integriert ist wurde von Kazemi and Sullivan in ihrem Paper: One Millisecond Face Alignment with an Ensemble of Regression Trees entwickelt.\cite{Kazemi2014OneMF} \\\\
Dieser Methode werden viele Trainingsdaten hinzugefügt womit es ein Kombination von Regressionsbäumen trainiert wird um die Positionen der „Facial Landmarks“ direkt aus den Pixelintensitäten selbst zu beurteilen.\cite{Kazemi2014OneMF} \\


Dadurch werden unsere gewünschten 68 Gesichtsmarkierungen mit hohen Vorhersagbarkeit extrahiert und als x,y Koordinaten weitergegeben. 
Die Indizes der 68 Koordinaten sind in der Abb.\ref{fig:landmarks} dargestellt:\\

\begin{figure}[H]
	\includegraphics[scale=0.8]{\ordnerfigures landmarks.png}
	\centering
	\caption{Facial Landmarks\cite{Kazemi2014OneMF}
\end{figure}

Nachdem wir der „Predictor“ geladen haben speichern wir sie in einem Formobjekt mit 68(x,y) Koordinaten. \\
Es wird hier nicht die resize Funktion verwendet, weil es an Qualität verliert und es rechenintensiver ist, je größer das Eingabebild wird. \\
Jede Koordinate läuft in einer Schleife durch und entspricht dem spezifischen Gesichtsmerkmal im Bild.  \\
Die Merkmale werden in einer numpy Array gespeichert für den Zweck der Speicherung in der Datenbank. \\\\
Nur die Positionen von diesen Merkmalen alleine reichen bei Gesichtserkennung nicht. Deswegen wurden die relative Positionen bzw. Abstände zwischen Gesichtsmerkmalen als Vektoren genommen und betrachtet um die Qualität und die Präzision der Erkennung zu verbessern. \\\\
Die Zahlen beziehen sich auf die \ref{fig:landmarks} genannten Indizes. \\
\begin{enumerate}
	\item Rechtes Auge Höhe: V1 = 41 – 37 
	\item Rechtes Auge Breite: V2 = 39 – 36
	\item Linkes Auge Höhe: V3 = 46-44
	\item Linkes Auge Breite: V4 = 45-42
	\item Rechte Augenbraue Breite: V5 = 21 – 17
	\item Linke Augenbraue Breite: V6 = 26 – 22 
	\item Rechte augenecke mit mittlerer rechter Augenbraue: V7 = 36 – 19 
	\item Linkes augenecke mit mittlerer linker Augenbraue: V8 = 45 – 24 
	\item Nasespitze mit Mundzentrum: V9 = 45 -24 
	\item Linke Augenecke mit linker Mund Ecke: V10 = 54 – 45
	\item Rechte Augenecke mit rechter Mund ecke: V11 = 48 – 36 
	\item Nase Höhe: V12 = 33 – 27 
	\item Nase Breite: V13 = 35 – 31 
\end{enumerate}







\section{Herausforderungen, Probleme und deren Lösung }

Die größte Herausforderung lag bei der Planung von der Software und die Methoden die zum Extrahieren von den Gesichtsschlüsselpunkten dienten.\\
 Es hat mich viel Zeit gekostet bis ich eine geeignete Lösung gefunden habe und das hat viel Stress gemacht. \\
Eine weitere Herausforderung war das Verknüpfen von den Entwicklungsumgebungen  und die Kooperation zwischen den Teammitgliedern. \\
Die verwendete Systeme waren all zu unterschiedlich und es konnte keine Standardisierung zwischen ihnen gefunden werden. Also ist viel Zeit beim Installieren und Konfigurieren investiert worden. Ein Grund dafür ist die mangelnde Erfahrung mit den neuen Technologien. \\
Viele Sachen waren am Beginn auch unklar wegen den Kommunikationslücken und auch die sehr grobe Planung war problematisch. \\\\

\textbf{Lösungen}: \\\\
Während der Arbeit habe ich viel recherchiert und mich genau über alles Mögliche informiert. \\
Die Kommunikation hat sich mit der Zeit stark verbessert und dadurch wurden auch die Unklarheiten abgeklärt. \\

\section{Qualitätssicherung, Controlling}
Die Qualität wurde durch verschiedene Methoden gesichert. Eine von denen war die Methode 5xWarum. 
“5x Warum ist eine iterative Fragetechnik, mit der die Ursache-Wirkungs-Beziehungen untersucht werden, die einem bestimmten Problem zugrunde liegen. “
\cite{fmea}
Die aufgetauchten Probleme haben viele Ursachen, die nicht mit dem ersten Blick sichtbar sind. 5x warum hilft durch diese verschachtelten Ursachen das grundlegende Problem zu entdecken. \\
Eine Problemstellung: „Segmentation fault“ beim Finden von Keypoints mit FAST\footnote{Features from Accelerated Segment Test}.“\\ 

\begin{enumerate}
	\item 	Warum bekommt man überhaupt einen „Segmentation Fault“? 
	Ein Segmentierungsfehler tritt auf, wenn ein Programm versucht, auf einen Speicherort zuzugreifen, auf den es nicht zugreifen darf, oder wenn versucht wird, auf einen Speicherort auf nicht zulässige Weise zuzugreifen (z. B. beim Versuch, an einen schreibgeschützten Speicherort zu schreiben, oder einen Teil des Betriebssystems zu überschreiben).\\
	\item 	Warum versucht mein Program auf einen Speicherort zuzugreifen, auf den es nicht zugreifen darf?\\
	
	Problem ergibt sich in dieser Zeile: 
	“kp = fast.detect(image, None)”
	
	Wahrscheinlich durften die Daten die fast.detect ergibt nicht in kp gespeichert werden. 
	
	\item	Warum dürfen die Daten die fast.detect ergibt nicht in kp gespeichert werden ? 
	
	Die Daten die fast.detect ergibt, dürfen nicht in kp gespeichert werden, weil kp kein Array ist. 
	
	\item	Warum ist kp kein Array? 
	
	Kp ist kein array, weil man es in Python  manuell angeben muss.
	
	\item Warum wurde es nicht manuell angegeben? 
	
	Es wurde nicht manuell gegeben weil, die Methode fast.detect nicht gut recherchiert wurde. Man sollte mehr darüber in die Dokumentation nachschauen.
 
	
\end{enumerate}
\section{Ergebnise}
Ich habe ungefähr 100 Stunden Zeit bis jetzt für die Diplomarbeit investiert und folgendes erreicht.\\ 
Face Detection wurde fertig implementiert.\\
Bilder sind halbwegs normalisiert worden. \\
Gesichtsschlüsselpunkte wurden extrahiert und die Merkmalvektoren wurden angegeben.  \\


